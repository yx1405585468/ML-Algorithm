{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 理论知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 不同于逻辑回归sigmoid函数，**y = 1/(1+e^(-(ax+b)))**，softmax函数预测多个概率，对应y属于每一类别的概率，如y=[0.7,0.2,0.1]\n",
    "2. 对应的softmax公式如下：\n",
    "3. ![Example Image](img/softmax公式1.jpg)\n",
    "4. https://blog.csdn.net/xu380393916/article/details/102496419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>花萼长度</th>\n",
       "      <th>花萼宽度</th>\n",
       "      <th>花瓣长度</th>\n",
       "      <th>花瓣宽度</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     花萼长度  花萼宽度  花瓣长度  花瓣宽度\n",
       "0     5.1   3.5   1.4   0.2\n",
       "1     4.9   3.0   1.4   0.2\n",
       "2     4.7   3.2   1.3   0.2\n",
       "3     4.6   3.1   1.5   0.2\n",
       "4     5.0   3.6   1.4   0.2\n",
       "..    ...   ...   ...   ...\n",
       "145   6.7   3.0   5.2   2.3\n",
       "146   6.3   2.5   5.0   1.9\n",
       "147   6.5   3.0   5.2   2.0\n",
       "148   6.2   3.4   5.4   2.3\n",
       "149   5.9   3.0   5.1   1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 处理数据集\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "y = pd.DataFrame(data=iris.target, columns=[\"label\"])\n",
    "\n",
    "# 特征重命名\n",
    "map_ = {\n",
    "    \"sepal length (cm)\": \"花萼长度\",\n",
    "    \"sepal width (cm)\": \"花萼宽度\",\n",
    "    \"petal length (cm)\": \"花瓣长度\",\n",
    "    \"petal width (cm)\": \"花瓣宽度\",\n",
    "}\n",
    "X = X.rename(columns=map_)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. softmax回归基于线性回归y=ax+b，y = e^(ax+b)/sum(e^(ax+b))，**添加xn+1列=1**，以融合b到参数A，有y = e^(X·A)/sum(e^(X·A))\n",
    "\n",
    "2. 由于softmax输出y对应每个标签的概率，如[0.7, 0.2, 0.1], 那么原始标签y也要处理成这样的形式，故此需要onehot编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意！！一定要对数据进行归一化，统一量纲，不然梯度下降大概率无法收敛！！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# 数据归一化！！！！非常重要！！！！\n",
    "X_1 = Normalizer().fit_transform(X)\n",
    "\n",
    "# 添加补充列\n",
    "X_1 = np.hstack((X_1, np.ones((len(X_1), 1))))\n",
    "\n",
    "# 将y array化，方便后续计算，矩阵乘法@符号，和A.dot(B)形式都要求为array，否则只能用np.dot(A,B)\n",
    "y = y.values\n",
    "\n",
    "# 对标签进行one-hot编码,[0. 0. 0. 1.]，即每个标签在对应标签处为1，其余为0\n",
    "n_classes = np.unique(y).shape[0]\n",
    "n_samples = X_1.shape[0]\n",
    "n_features = X_1.shape[1]\n",
    "\n",
    "y_one_hot = np.zeros((n_samples, n_classes))\n",
    "y_one_hot[np.arange(n_samples), y.T] = 1\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 算法实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 梯度下降\n",
    "**非凸函数，不可正规方程求解**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先，能够理解希望函数能输出每一个类别的概率，故此softmax函数有归一化操作：**X_1@A / np.sum(X_1@A,axis=1,keepdims=True)**\n",
    "\n",
    "2. 为什么要将**X_1@A**指数化呢？**y = np.exp(X_1@A) / np.sum(np.exp(X_1@A),axis=1,keepdims=True)** \n",
    "\n",
    "3. 这么做主要有两个原因\n",
    "\n",
    "    1. 通过将输出指数化，将假设函数的输出转换为正值。这对于创建一个有效的概率分布是必需的，因为概率值不能为负。\n",
    "    \n",
    "    2. 指数函数曲线呈现递增趋势，最重要的是斜率逐渐增大，也就是说在x轴上一个很小的变化，可以导致y轴上很大的变化。这有助于在计算概率时清晰地区分那些具有更高原始逻辑值的类别。\n",
    "\n",
    "4. 交叉熵损失: ![Example Image](img/交叉熵损失1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.84552377, 0.74903728, 1.52999726],\n",
       "       [0.8229153 , 0.75277879, 1.5495243 ],\n",
       "       [0.84539864, 0.74958422, 1.53420451],\n",
       "       [0.84083251, 0.75909509, 1.56489725],\n",
       "       [0.85503518, 0.7489956 , 1.52795113],\n",
       "       [0.86887447, 0.75505298, 1.56777929],\n",
       "       [0.87004417, 0.75261248, 1.55349934],\n",
       "       [0.84266613, 0.75416084, 1.54671312],\n",
       "       [0.83785721, 0.75804367, 1.56412446],\n",
       "       [0.81856447, 0.7564913 , 1.54581977],\n",
       "       [0.84402836, 0.74973185, 1.53082031],\n",
       "       [0.84935586, 0.75928401, 1.56175255],\n",
       "       [0.81713543, 0.75385755, 1.53861298],\n",
       "       [0.84221167, 0.7449588 , 1.50685674],\n",
       "       [0.84692225, 0.7345615 , 1.48395147],\n",
       "       [0.88261838, 0.74317289, 1.52607541],\n",
       "       [0.87284429, 0.74050299, 1.52393168],\n",
       "       [0.85456098, 0.74883717, 1.54185843],\n",
       "       [0.84469723, 0.75407277, 1.55561574],\n",
       "       [0.86935275, 0.75053773, 1.54277765],\n",
       "       [0.82498974, 0.75810338, 1.56180659],\n",
       "       [0.87294526, 0.75100147, 1.55783432],\n",
       "       [0.8751626 , 0.73345496, 1.48065759],\n",
       "       [0.85705678, 0.76002309, 1.60586894],\n",
       "       [0.84446741, 0.76942258, 1.59503805],\n",
       "       [0.81638283, 0.75941415, 1.57120083],\n",
       "       [0.85957985, 0.75728439, 1.58202485],\n",
       "       [0.84058053, 0.75215671, 1.54011943],\n",
       "       [0.83581551, 0.74881126, 1.53160237],\n",
       "       [0.84143726, 0.76141306, 1.57115536],\n",
       "       [0.83101064, 0.76108841, 1.57264977],\n",
       "       [0.84454196, 0.75049761, 1.56245701],\n",
       "       [0.86279363, 0.74770324, 1.50809174],\n",
       "       [0.86597008, 0.74216021, 1.50270979],\n",
       "       [0.82828669, 0.75640987, 1.55859245],\n",
       "       [0.83357344, 0.74294907, 1.51636464],\n",
       "       [0.83083649, 0.74224406, 1.51228474],\n",
       "       [0.84970919, 0.74960186, 1.5169043 ],\n",
       "       [0.8459848 , 0.75320177, 1.54704506],\n",
       "       [0.83870943, 0.75341051, 1.54500914],\n",
       "       [0.85966804, 0.74547456, 1.5313819 ],\n",
       "       [0.79945718, 0.7526401 , 1.58114097],\n",
       "       [0.85862845, 0.75165333, 1.53899633],\n",
       "       [0.88250806, 0.75554697, 1.60085575],\n",
       "       [0.87240847, 0.76386829, 1.59698229],\n",
       "       [0.83692258, 0.75351448, 1.56455414],\n",
       "       [0.85942441, 0.75435772, 1.5424946 ],\n",
       "       [0.84850797, 0.75447   , 1.5486433 ],\n",
       "       [0.84777422, 0.75035856, 1.53219545],\n",
       "       [0.83785588, 0.75081871, 1.53813262],\n",
       "       [0.79389807, 0.78991296, 1.82591231],\n",
       "       [0.81762593, 0.79078041, 1.84428904],\n",
       "       [0.79159488, 0.79003824, 1.84408537],\n",
       "       [0.78425853, 0.78753525, 1.85888322],\n",
       "       [0.79012854, 0.78800247, 1.8511229 ],\n",
       "       [0.79774242, 0.79351171, 1.86071659],\n",
       "       [0.82579176, 0.79175436, 1.85904109],\n",
       "       [0.80721546, 0.79111359, 1.82467646],\n",
       "       [0.78194276, 0.79032843, 1.83260827],\n",
       "       [0.82901573, 0.79057881, 1.86662751],\n",
       "       [0.76722298, 0.78787461, 1.83783995],\n",
       "       [0.82645177, 0.78997238, 1.85364856],\n",
       "       [0.74478244, 0.78602892, 1.81540658],\n",
       "       [0.7958506 , 0.79231846, 1.85945508],\n",
       "       [0.83274941, 0.78843124, 1.82533307],\n",
       "       [0.80181371, 0.78884316, 1.82557574],\n",
       "       [0.82469443, 0.79263157, 1.87412306],\n",
       "       [0.78048037, 0.79339271, 1.82088179],\n",
       "       [0.76131311, 0.78240715, 1.86467994],\n",
       "       [0.78450113, 0.790709  , 1.83157704],\n",
       "       [0.83776046, 0.79012861, 1.88904524],\n",
       "       [0.80222986, 0.78834259, 1.82761326],\n",
       "       [0.7683639 , 0.78731419, 1.87092459],\n",
       "       [0.77744305, 0.79357977, 1.84666609],\n",
       "       [0.79355052, 0.78954287, 1.82788801],\n",
       "       [0.79844894, 0.78876273, 1.83052593],\n",
       "       [0.77332666, 0.78848772, 1.84109188],\n",
       "       [0.79881402, 0.78855952, 1.86760275],\n",
       "       [0.80969418, 0.79067712, 1.8631675 ],\n",
       "       [0.79228374, 0.78812966, 1.79799118],\n",
       "       [0.7829334 , 0.789729  , 1.83281874],\n",
       "       [0.77845156, 0.79003249, 1.8196504 ],\n",
       "       [0.79959646, 0.78989913, 1.82847128],\n",
       "       [0.78661313, 0.78961302, 1.8901662 ],\n",
       "       [0.8292601 , 0.79301849, 1.88033021],\n",
       "       [0.84360042, 0.79202478, 1.85890512],\n",
       "       [0.80048625, 0.78991562, 1.84360828],\n",
       "       [0.75575343, 0.78482949, 1.84238541],\n",
       "       [0.82420318, 0.79316189, 1.84530166],\n",
       "       [0.79800493, 0.78959691, 1.85568619],\n",
       "       [0.78570927, 0.79350836, 1.86030013],\n",
       "       [0.80444368, 0.7925946 , 1.85441821],\n",
       "       [0.79040364, 0.78986163, 1.83458005],\n",
       "       [0.79675346, 0.78947792, 1.82259604],\n",
       "       [0.80267978, 0.79189893, 1.8557893 ],\n",
       "       [0.81220544, 0.79446416, 1.83822935],\n",
       "       [0.81273607, 0.79278934, 1.84813733],\n",
       "       [0.79814966, 0.79077255, 1.83481187],\n",
       "       [0.82503379, 0.7850534 , 1.80513574],\n",
       "       [0.80949721, 0.79153637, 1.84637627],\n",
       "       [0.83515051, 0.78271403, 1.93922329],\n",
       "       [0.80772413, 0.78580727, 1.91507536],\n",
       "       [0.79010137, 0.78545887, 1.90071403],\n",
       "       [0.79014607, 0.78895526, 1.90162924],\n",
       "       [0.80756351, 0.78470897, 1.92065871],\n",
       "       [0.76641216, 0.78536059, 1.90002133],\n",
       "       [0.82187499, 0.78655269, 1.92171822],\n",
       "       [0.75757477, 0.7876803 , 1.88745741],\n",
       "       [0.75583621, 0.78425407, 1.89767928],\n",
       "       [0.83160121, 0.7854302 , 1.91412159],\n",
       "       [0.82718465, 0.78736676, 1.89123688],\n",
       "       [0.79073664, 0.78528652, 1.90075878],\n",
       "       [0.80530168, 0.78516245, 1.90076032],\n",
       "       [0.80627744, 0.78210093, 1.92489947],\n",
       "       [0.84053635, 0.77844969, 1.94072321],\n",
       "       [0.83894886, 0.78393926, 1.9153718 ],\n",
       "       [0.79502002, 0.7894529 , 1.89260514],\n",
       "       [0.80495763, 0.79056523, 1.8961628 ],\n",
       "       [0.74753887, 0.77886627, 1.91344346],\n",
       "       [0.75182621, 0.78491972, 1.88619286],\n",
       "       [0.81854695, 0.78455922, 1.91007632],\n",
       "       [0.82968581, 0.78487664, 1.92108394],\n",
       "       [0.74815723, 0.78413692, 1.8950454 ],\n",
       "       [0.79796017, 0.78572187, 1.88796763],\n",
       "       [0.81742303, 0.78812283, 1.90374842],\n",
       "       [0.78149006, 0.79034643, 1.88157032],\n",
       "       [0.80866564, 0.78659116, 1.88709973],\n",
       "       [0.8194117 , 0.78885063, 1.88982804],\n",
       "       [0.79881791, 0.78407383, 1.91703302],\n",
       "       [0.76547874, 0.79019727, 1.86764988],\n",
       "       [0.76042574, 0.7852833 , 1.88715936],\n",
       "       [0.79974529, 0.79172192, 1.87580885],\n",
       "       [0.80396339, 0.78267183, 1.92235334],\n",
       "       [0.78117857, 0.79088659, 1.87293381],\n",
       "       [0.75306518, 0.79061605, 1.88540623],\n",
       "       [0.78535567, 0.78225311, 1.89800952],\n",
       "       [0.84721321, 0.78446955, 1.92579601],\n",
       "       [0.80219979, 0.79039869, 1.89394726],\n",
       "       [0.82447024, 0.78872252, 1.89017999],\n",
       "       [0.8112615 , 0.78545854, 1.89408154],\n",
       "       [0.82453248, 0.78231438, 1.9199531 ],\n",
       "       [0.8287533 , 0.78140324, 1.89654535],\n",
       "       [0.80772413, 0.78580727, 1.91507536],\n",
       "       [0.8150556 , 0.7850486 , 1.91681299],\n",
       "       [0.83666799, 0.78266163, 1.92383724],\n",
       "       [0.82507107, 0.78155428, 1.90699067],\n",
       "       [0.78890132, 0.78256312, 1.8989006 ],\n",
       "       [0.81375635, 0.78623542, 1.89709412],\n",
       "       [0.85021451, 0.78549771, 1.91927922],\n",
       "       [0.81754562, 0.78966244, 1.90091203]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "对理论开始验证, 看下softmax函数每步操作的结果是否具有必要性以及原理性\n",
    "\"\"\"\n",
    "\n",
    "# 1. 首先，将y分别输出三类的概率，这样的思路是没问题的, 那么，初始化系数A\n",
    "A = np.random.rand(n_features, n_classes)\n",
    "\n",
    "# 很显然X·A的操作并不能实现输出概率化(3类加起来 = 1)\n",
    "X_1 @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27060585, 0.23972581, 0.48966833],\n",
       "       [0.26331449, 0.24087238, 0.49581313],\n",
       "       [0.27016555, 0.23954597, 0.49028848],\n",
       "       [0.26568058, 0.23985374, 0.49446567],\n",
       "       [0.27300132, 0.23914429, 0.48785439],\n",
       "       [0.27222879, 0.23656715, 0.49120405],\n",
       "       [0.27392993, 0.23695703, 0.48911305],\n",
       "       [0.26806279, 0.23990814, 0.49202907],\n",
       "       [0.26514256, 0.23988531, 0.49497213],\n",
       "       [0.2622868 , 0.24239714, 0.49531606],\n",
       "       [0.27012534, 0.2399464 , 0.48992826],\n",
       "       [0.26790244, 0.23949212, 0.49260544],\n",
       "       [0.2627778 , 0.24242864, 0.49479355],\n",
       "       [0.27220564, 0.24077319, 0.48702117],\n",
       "       [0.27628124, 0.23962715, 0.48409161],\n",
       "       [0.28003037, 0.23578818, 0.48418146],\n",
       "       [0.27821698, 0.23603352, 0.4857495 ],\n",
       "       [0.27169834, 0.2380846 , 0.49021706],\n",
       "       [0.26778501, 0.23905534, 0.49315964],\n",
       "       [0.27487954, 0.23731157, 0.4878089 ],\n",
       "       [0.26232625, 0.24105805, 0.4966157 ],\n",
       "       [0.27435743, 0.23603179, 0.48961079],\n",
       "       [0.2832906 , 0.23741976, 0.47928965],\n",
       "       [0.26592317, 0.23581606, 0.49826076],\n",
       "       [0.26316184, 0.23977558, 0.49706258],\n",
       "       [0.25941639, 0.24131385, 0.49926976],\n",
       "       [0.26871199, 0.23673356, 0.49455445],\n",
       "       [0.2683112 , 0.24008654, 0.49160226],\n",
       "       [0.26821375, 0.24029403, 0.49149222],\n",
       "       [0.26510263, 0.23989027, 0.4950071 ],\n",
       "       [0.26258344, 0.24048936, 0.4969272 ],\n",
       "       [0.26747201, 0.23768755, 0.49484044],\n",
       "       [0.27666157, 0.23975693, 0.48358149],\n",
       "       [0.27837178, 0.23857228, 0.48305594],\n",
       "       [0.26350955, 0.2406428 , 0.49584764],\n",
       "       [0.26951305, 0.24021215, 0.4902748 ],\n",
       "       [0.26928302, 0.24056926, 0.49014771],\n",
       "       [0.27267345, 0.2405488 , 0.48677775],\n",
       "       [0.26888828, 0.23939807, 0.49171366],\n",
       "       [0.26734935, 0.24015923, 0.49249142],\n",
       "       [0.274083  , 0.23767535, 0.48824165],\n",
       "       [0.25515365, 0.24021158, 0.50463477],\n",
       "       [0.27264294, 0.2386748 , 0.48868225],\n",
       "       [0.27247063, 0.23327193, 0.49425744],\n",
       "       [0.26982325, 0.23625335, 0.4939234 ],\n",
       "       [0.26526939, 0.23883251, 0.4958981 ],\n",
       "       [0.27229058, 0.2390024 , 0.48870702],\n",
       "       [0.26922904, 0.23939107, 0.49137989],\n",
       "       [0.27082598, 0.23970603, 0.48946798],\n",
       "       [0.26795892, 0.24012312, 0.49191796],\n",
       "       [0.23283357, 0.23166483, 0.5355016 ],\n",
       "       [0.23680801, 0.22903278, 0.53415921],\n",
       "       [0.23107412, 0.23061972, 0.53830616],\n",
       "       [0.22860168, 0.22955681, 0.54184151],\n",
       "       [0.23040829, 0.22978831, 0.53980339],\n",
       "       [0.23109768, 0.22987209, 0.53903023],\n",
       "       [0.23752943, 0.22773896, 0.53473161],\n",
       "       [0.23582067, 0.23111666, 0.53306267],\n",
       "       [0.22965358, 0.23211642, 0.53823   ],\n",
       "       [0.23779774, 0.22677236, 0.5354299 ],\n",
       "       [0.22612352, 0.23221017, 0.54166631],\n",
       "       [0.23816555, 0.22765298, 0.53418147],\n",
       "       [0.2225744 , 0.23490069, 0.54252491],\n",
       "       [0.2308403 , 0.22981579, 0.53934391],\n",
       "       [0.2416208 , 0.22876196, 0.52961724],\n",
       "       [0.234707  , 0.23091026, 0.53438274],\n",
       "       [0.23620405, 0.2270208 , 0.53677514],\n",
       "       [0.22990773, 0.23371134, 0.53638093],\n",
       "       [0.22336377, 0.22955261, 0.54708363],\n",
       "       [0.23027594, 0.23209815, 0.53762591],\n",
       "       [0.23820759, 0.22466402, 0.53712838],\n",
       "       [0.23469464, 0.23063188, 0.53467348],\n",
       "       [0.2242349 , 0.22976524, 0.54599986],\n",
       "       [0.22747625, 0.23219778, 0.54032597],\n",
       "       [0.23264581, 0.23147088, 0.53588331],\n",
       "       [0.23361915, 0.23078505, 0.53559581],\n",
       "       [0.22725476, 0.23171009, 0.54103514],\n",
       "       [0.2312068 , 0.22823876, 0.54055443],\n",
       "       [0.23377656, 0.22828591, 0.53793753],\n",
       "       [0.23451417, 0.23328457, 0.53220126],\n",
       "       [0.2299039 , 0.23189939, 0.53819671],\n",
       "       [0.22975817, 0.23317625, 0.53706558],\n",
       "       [0.23393921, 0.23110204, 0.53495875],\n",
       "       [0.22692559, 0.22779101, 0.5452834 ],\n",
       "       [0.23675499, 0.22640795, 0.53683706],\n",
       "       [0.24140595, 0.22664699, 0.53194706],\n",
       "       [0.23310538, 0.23002716, 0.53686745],\n",
       "       [0.2233995 , 0.23199434, 0.54460616],\n",
       "       [0.23802556, 0.229061  , 0.53291345],\n",
       "       [0.23175666, 0.2293148 , 0.53892854],\n",
       "       [0.22843588, 0.23070338, 0.54086074],\n",
       "       [0.23307368, 0.22964062, 0.5372857 ],\n",
       "       [0.23146104, 0.23130232, 0.53723665],\n",
       "       [0.23373241, 0.23159809, 0.53466949],\n",
       "       [0.23263599, 0.22951144, 0.53785257],\n",
       "       [0.23577047, 0.23062045, 0.53360908],\n",
       "       [0.23532584, 0.22955031, 0.53512386],\n",
       "       [0.23312256, 0.23096787, 0.53590957],\n",
       "       [0.24157538, 0.22986886, 0.52855576],\n",
       "       [0.23481316, 0.22960321, 0.53558363],\n",
       "       [0.2347849 , 0.22004349, 0.54517161],\n",
       "       [0.23021221, 0.22396561, 0.54582217],\n",
       "       [0.22728396, 0.22594848, 0.54676757],\n",
       "       [0.22700581, 0.2266637 , 0.54633049],\n",
       "       [0.2298831 , 0.22337727, 0.54673963],\n",
       "       [0.22203299, 0.22752243, 0.55044458],\n",
       "       [0.23281615, 0.22281025, 0.5443736 ],\n",
       "       [0.22069275, 0.22946294, 0.54984431],\n",
       "       [0.21986238, 0.22812875, 0.55200887],\n",
       "       [0.23550416, 0.22242882, 0.54206702],\n",
       "       [0.23594826, 0.2245905 , 0.53946123],\n",
       "       [0.22743349, 0.22586591, 0.5467006 ],\n",
       "       [0.23066454, 0.22489601, 0.54443945],\n",
       "       [0.22949436, 0.22261289, 0.54789275],\n",
       "       [0.236125  , 0.2186835 , 0.5451915 ],\n",
       "       [0.23710775, 0.22156068, 0.54133157],\n",
       "       [0.22864601, 0.22704492, 0.54430907],\n",
       "       [0.23053554, 0.22641363, 0.54305083],\n",
       "       [0.21731738, 0.22642458, 0.55625805],\n",
       "       [0.21964349, 0.22931164, 0.55104487],\n",
       "       [0.23299301, 0.22331866, 0.54368833],\n",
       "       [0.23466312, 0.22198957, 0.54334731],\n",
       "       [0.21829096, 0.2287888 , 0.55292024],\n",
       "       [0.22985043, 0.22632522, 0.54382435],\n",
       "       [0.23293089, 0.22458157, 0.54248754],\n",
       "       [0.2262954 , 0.22885993, 0.54484468],\n",
       "       [0.23221793, 0.22587899, 0.54190308],\n",
       "       [0.23424544, 0.22550893, 0.54024563],\n",
       "       [0.22823859, 0.22402591, 0.54773549],\n",
       "       [0.22360674, 0.23082736, 0.5455659 ],\n",
       "       [0.22151322, 0.22875427, 0.54973251],\n",
       "       [0.23065521, 0.22834118, 0.54100361],\n",
       "       [0.22911542, 0.2230477 , 0.54783688],\n",
       "       [0.22675727, 0.22957527, 0.54366745],\n",
       "       [0.21961096, 0.23056165, 0.5498274 ],\n",
       "       [0.22661344, 0.2257182 , 0.54766837],\n",
       "       [0.2381499 , 0.22051278, 0.54133732],\n",
       "       [0.2300844 , 0.22669965, 0.54321595],\n",
       "       [0.23533615, 0.22513234, 0.53953151],\n",
       "       [0.23239977, 0.22500807, 0.54259215],\n",
       "       [0.23379054, 0.22181989, 0.54438957],\n",
       "       [0.23633412, 0.22283139, 0.5408345 ],\n",
       "       [0.23021221, 0.22396561, 0.54582217],\n",
       "       [0.23175286, 0.22322067, 0.54502648],\n",
       "       [0.23613565, 0.22089325, 0.54297111],\n",
       "       [0.23482107, 0.22243588, 0.54274305],\n",
       "       [0.22732517, 0.22549879, 0.54717604],\n",
       "       [0.23269556, 0.22482588, 0.54247856],\n",
       "       [0.23916078, 0.22095629, 0.53988294],\n",
       "       [0.2330438 , 0.22509561, 0.54186059]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 归一化：有效果\n",
    "X_1 @ A / np.sum(X_1 @ A, axis=1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25701987, 0.23337974, 0.50960039],\n",
       "       [0.24997985, 0.23304785, 0.5169723 ],\n",
       "       [0.25641231, 0.23298459, 0.51060311],\n",
       "       [0.25098418, 0.23128541, 0.51773041],\n",
       "       [0.25911212, 0.2330426 , 0.50784527],\n",
       "       [0.25614979, 0.22859249, 0.51525772],\n",
       "       [0.25840389, 0.22977306, 0.51182306],\n",
       "       [0.2539865 , 0.2324734 , 0.51354009],\n",
       "       [0.25058651, 0.23136364, 0.51804985],\n",
       "       [0.24942588, 0.23441396, 0.51616016],\n",
       "       [0.25658508, 0.23349573, 0.50991919],\n",
       "       [0.25298923, 0.23119813, 0.51581264],\n",
       "       [0.25024063, 0.23489653, 0.51486284],\n",
       "       [0.25966425, 0.23560026, 0.50473549],\n",
       "       [0.26423032, 0.2361484 , 0.49962128],\n",
       "       [0.26504889, 0.23055024, 0.50440087],\n",
       "       [0.26359721, 0.23092222, 0.50548057],\n",
       "       [0.25720113, 0.23139694, 0.51140193],\n",
       "       [0.25321449, 0.23127616, 0.51550935],\n",
       "       [0.25981355, 0.23070717, 0.50947927],\n",
       "       [0.24847338, 0.23239753, 0.51912909],\n",
       "       [0.25848143, 0.22880728, 0.5127113 ],\n",
       "       [0.2702678 , 0.23455865, 0.49517355],\n",
       "       [0.24863176, 0.22563965, 0.52572859],\n",
       "       [0.24716317, 0.22929374, 0.5235431 ],\n",
       "       [0.24558873, 0.23198893, 0.52242234],\n",
       "       [0.25238343, 0.22784237, 0.5197742 ],\n",
       "       [0.2545697 , 0.23302619, 0.5124041 ],\n",
       "       [0.25497458, 0.23372837, 0.51129705],\n",
       "       [0.25015073, 0.23091264, 0.51893663],\n",
       "       [0.2480257 , 0.23127562, 0.52069868],\n",
       "       [0.25249996, 0.22983617, 0.51766387],\n",
       "       [0.2633075 , 0.23468218, 0.50201032],\n",
       "       [0.26498099, 0.23412337, 0.50089564],\n",
       "       [0.24960309, 0.232292  , 0.51810492],\n",
       "       [0.25688743, 0.23463089, 0.50848168],\n",
       "       [0.25694015, 0.23515638, 0.50790347],\n",
       "       [0.25950408, 0.23478381, 0.50571211],\n",
       "       [0.25462915, 0.23206677, 0.51330408],\n",
       "       [0.25350364, 0.23277661, 0.51371975],\n",
       "       [0.2597622 , 0.23173005, 0.50850775],\n",
       "       [0.2415803 , 0.23053088, 0.52788882],\n",
       "       [0.25818764, 0.231994  , 0.50981836],\n",
       "       [0.25433583, 0.22401084, 0.52165333],\n",
       "       [0.25246187, 0.22649436, 0.52104377],\n",
       "       [0.25061731, 0.23056182, 0.51882087],\n",
       "       [0.2577179 , 0.23201427, 0.51026784],\n",
       "       [0.25482424, 0.23195331, 0.51322245],\n",
       "       [0.25708244, 0.23321975, 0.50969781],\n",
       "       [0.25439123, 0.23318595, 0.51242282],\n",
       "       [0.20821463, 0.20738653, 0.58439884],\n",
       "       [0.20985275, 0.20429409, 0.58585317],\n",
       "       [0.20562502, 0.20530518, 0.5890698 ],\n",
       "       [0.20275028, 0.20341573, 0.59383398],\n",
       "       [0.20461954, 0.20418497, 0.59119549],\n",
       "       [0.20446776, 0.20360455, 0.59192769],\n",
       "       [0.20934878, 0.202343  , 0.58830822],\n",
       "       [0.21051791, 0.20715532, 0.58232676],\n",
       "       [0.20542425, 0.20715411, 0.58742164],\n",
       "       [0.20899687, 0.2011161 , 0.58988703],\n",
       "       [0.20250929, 0.20673492, 0.59075579],\n",
       "       [0.21019872, 0.20266898, 0.5871323 ],\n",
       "       [0.20164101, 0.2101319 , 0.58822709],\n",
       "       [0.20436241, 0.20364185, 0.59199574],\n",
       "       [0.21482987, 0.20551689, 0.57965324],\n",
       "       [0.20961019, 0.20690899, 0.58348083],\n",
       "       [0.20727689, 0.20073641, 0.5919867 ],\n",
       "       [0.20646871, 0.20915199, 0.58437931],\n",
       "       [0.19858545, 0.20281891, 0.59859564],\n",
       "       [0.20595055, 0.20723304, 0.58681641],\n",
       "       [0.20769253, 0.19803166, 0.59427581],\n",
       "       [0.20945159, 0.20656298, 0.58398543],\n",
       "       [0.19876766, 0.20257028, 0.59866207],\n",
       "       [0.20286515, 0.20616528, 0.59096956],\n",
       "       [0.20793297, 0.20710131, 0.58496572],\n",
       "       [0.20845275, 0.20644338, 0.58510387],\n",
       "       [0.20308081, 0.20618319, 0.590736  ],\n",
       "       [0.20401301, 0.20193165, 0.59405535],\n",
       "       [0.20623844, 0.20235345, 0.5914081 ],\n",
       "       [0.21143012, 0.21055364, 0.57801624],\n",
       "       [0.20558609, 0.20698793, 0.58742597],\n",
       "       [0.20642992, 0.20883446, 0.58473562],\n",
       "       [0.20884389, 0.20682845, 0.58432765],\n",
       "       [0.19928801, 0.19988675, 0.60082525],\n",
       "       [0.20724882, 0.19987226, 0.59287892],\n",
       "       [0.2123171 , 0.20164431, 0.58603859],\n",
       "       [0.20714447, 0.20496635, 0.58788919],\n",
       "       [0.20024926, 0.20615719, 0.59359356],\n",
       "       [0.21071803, 0.20427755, 0.58500441],\n",
       "       [0.20528345, 0.20356466, 0.59115189],\n",
       "       [0.20256759, 0.20415361, 0.59327881],\n",
       "       [0.20636431, 0.20393351, 0.58970218],\n",
       "       [0.20658955, 0.20647761, 0.58693284],\n",
       "       [0.20910897, 0.20759311, 0.58329792],\n",
       "       [0.2059381 , 0.20372984, 0.59033205],\n",
       "       [0.20953968, 0.20585496, 0.58460536],\n",
       "       [0.20848592, 0.20436851, 0.58714557],\n",
       "       [0.20779482, 0.20626754, 0.58593764],\n",
       "       [0.21619115, 0.20771825, 0.5760906 ],\n",
       "       [0.20822053, 0.2045141 , 0.58726538],\n",
       "       [0.20139604, 0.19110764, 0.60749631],\n",
       "       [0.19981404, 0.19548239, 0.60470357],\n",
       "       [0.19874448, 0.19782395, 0.60343157],\n",
       "       [0.19850438, 0.19826814, 0.60322747],\n",
       "       [0.19915679, 0.19465677, 0.60618644],\n",
       "       [0.1950847 , 0.1988165 , 0.6060988 ],\n",
       "       [0.20124817, 0.1942637 , 0.60448813],\n",
       "       [0.19508935, 0.20105192, 0.60385873],\n",
       "       [0.19374731, 0.19933217, 0.60692052],\n",
       "       [0.20379095, 0.19459563, 0.60161342],\n",
       "       [0.20580132, 0.19776775, 0.59643094],\n",
       "       [0.19884707, 0.19776628, 0.60338665],\n",
       "       [0.2011823 , 0.19717117, 0.60164653],\n",
       "       [0.19854083, 0.19379837, 0.6076608 ],\n",
       "       [0.20224374, 0.19006896, 0.60768731],\n",
       "       [0.20489111, 0.19392454, 0.60118435],\n",
       "       [0.20034586, 0.19923361, 0.60042053],\n",
       "       [0.20146784, 0.198589  , 0.59994316],\n",
       "       [0.19081612, 0.19688851, 0.61229538],\n",
       "       [0.19444468, 0.2009872 , 0.60456812],\n",
       "       [0.20220772, 0.19545061, 0.60234167],\n",
       "       [0.20265004, 0.1937699 , 0.60358006],\n",
       "       [0.19286347, 0.19992898, 0.60720756],\n",
       "       [0.20152626, 0.19907495, 0.59939879],\n",
       "       [0.2026552 , 0.19680351, 0.60054129],\n",
       "       [0.1994693 , 0.20124372, 0.59928699],\n",
       "       [0.20332479, 0.19888568, 0.59778953],\n",
       "       [0.20464567, 0.19848608, 0.59686824],\n",
       "       [0.19822577, 0.19532456, 0.60644967],\n",
       "       [0.19858125, 0.20355106, 0.59786769],\n",
       "       [0.19566674, 0.2005915 , 0.60374176],\n",
       "       [0.20304076, 0.2014182 , 0.59554104],\n",
       "       [0.1984577 , 0.19427689, 0.60726541],\n",
       "       [0.20043075, 0.202386  , 0.59718325],\n",
       "       [0.19450799, 0.2019508 , 0.6035412 ],\n",
       "       [0.19843931, 0.1978246 , 0.60373609],\n",
       "       [0.2049309 , 0.19246787, 0.60260123],\n",
       "       [0.2012986 , 0.19893702, 0.59976437],\n",
       "       [0.20543236, 0.19821833, 0.59634931],\n",
       "       [0.20294174, 0.19777223, 0.59928603],\n",
       "       [0.2020569 , 0.19370401, 0.60423909],\n",
       "       [0.20564694, 0.19613648, 0.59821658],\n",
       "       [0.19981404, 0.19548239, 0.60470357],\n",
       "       [0.20080769, 0.19487157, 0.60432074],\n",
       "       [0.2035295 , 0.19282916, 0.60364134],\n",
       "       [0.20375919, 0.19508241, 0.6011584 ],\n",
       "       [0.19888473, 0.19762815, 0.60348712],\n",
       "       [0.20294765, 0.1974385 , 0.59961384],\n",
       "       [0.20618612, 0.19326503, 0.60054884],\n",
       "       [0.20295868, 0.19737772, 0.5996636 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据实验，不做e的指数化不可行，尽管归一化能得到总和为1，但是特殊的A会导致预测值为负数(尽管3类别的预测值相加仍为1)，不能得到我们想要的概率。\n",
    "prediction = np.exp(X_1 @ A) / np.sum(np.exp(X_1 @ A), axis=1, keepdims=True)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35860189, -1.45508838, -0.6741284 ],\n",
       "       [-1.38637498, -1.45651148, -0.65976598],\n",
       "       [-1.36096856, -1.45678298, -0.67216269],\n",
       "       [-1.38236536, -1.46410278, -0.65830062],\n",
       "       [-1.3504944 , -1.45653399, -0.67757846],\n",
       "       [-1.3619929 , -1.47581438, -0.66308807],\n",
       "       [-1.35323147, -1.47066317, -0.6697763 ],\n",
       "       [-1.37047416, -1.45897945, -0.66642717],\n",
       "       [-1.38395106, -1.4637646 , -0.65768381],\n",
       "       [-1.38859348, -1.45066665, -0.66133818],\n",
       "       [-1.36029497, -1.45459148, -0.67350302],\n",
       "       [-1.37440837, -1.46448022, -0.66201168],\n",
       "       [-1.38533229, -1.44861017, -0.66385474],\n",
       "       [-1.34836584, -1.44561871, -0.68372077],\n",
       "       [-1.33093412, -1.44329487, -0.69390491],\n",
       "       [-1.32784098, -1.46728647, -0.68438396],\n",
       "       [-1.33333306, -1.46567436, -0.68224567],\n",
       "       [-1.35789689, -1.4636207 , -0.67059944],\n",
       "       [-1.37351835, -1.46414281, -0.66259984],\n",
       "       [-1.347791  , -1.46660603, -0.67436611],\n",
       "       [-1.39241954, -1.4593059 , -0.6556027 ],\n",
       "       [-1.35293143, -1.47487522, -0.66804237],\n",
       "       [-1.30834196, -1.45004961, -0.70284697],\n",
       "       [-1.39178235, -1.48881603, -0.64297019],\n",
       "       [-1.39770657, -1.4727514 , -0.64713593],\n",
       "       [-1.40409695, -1.46106563, -0.64927895],\n",
       "       [-1.3768058 , -1.47910125, -0.65436079],\n",
       "       [-1.3681806 , -1.45660442, -0.6686417 ],\n",
       "       [-1.36659141, -1.45359566, -0.67080455],\n",
       "       [-1.38569162, -1.46571581, -0.65597351],\n",
       "       [-1.39422289, -1.46414512, -0.65258376],\n",
       "       [-1.37634419, -1.47038854, -0.65842914],\n",
       "       [-1.33443272, -1.4495231 , -0.68913461],\n",
       "       [-1.3280972 , -1.45190708, -0.6913575 ],\n",
       "       [-1.38788328, -1.45976009, -0.65757751],\n",
       "       [-1.3591173 , -1.44974167, -0.6763261 ],\n",
       "       [-1.35891211, -1.44750455, -0.67746386],\n",
       "       [-1.34898284, -1.44909016, -0.68178772],\n",
       "       [-1.36794712, -1.46073014, -0.66688686],\n",
       "       [-1.37237711, -1.45767603, -0.6660774 ],\n",
       "       [-1.34798868, -1.46218216, -0.67627483],\n",
       "       [-1.42055337, -1.46737045, -0.63886958],\n",
       "       [-1.35406865, -1.46104378, -0.67370078],\n",
       "       [-1.36909972, -1.49606082, -0.65075204],\n",
       "       [-1.37649505, -1.48503523, -0.65192123],\n",
       "       [-1.38382816, -1.46723626, -0.6561966 ],\n",
       "       [-1.35588972, -1.46095641, -0.67281952],\n",
       "       [-1.36718123, -1.4612192 , -0.66704589],\n",
       "       [-1.35835848, -1.45577414, -0.67393725],\n",
       "       [-1.36888191, -1.45591908, -0.66860517],\n",
       "       [-1.56918583, -1.57317093, -0.53717159],\n",
       "       [-1.5613492 , -1.58819472, -0.53468609],\n",
       "       [-1.58170108, -1.58325772, -0.52921059],\n",
       "       [-1.59578018, -1.59250346, -0.52115549],\n",
       "       [-1.58660291, -1.58872897, -0.52560855],\n",
       "       [-1.58734496, -1.59157567, -0.52437079],\n",
       "       [-1.56375361, -1.59779101, -0.53050428],\n",
       "       [-1.55818454, -1.57428641, -0.54072354],\n",
       "       [-1.58267793, -1.57429226, -0.53201242],\n",
       "       [-1.56543601, -1.60387293, -0.52782423],\n",
       "       [-1.59696953, -1.57631789, -0.52635256],\n",
       "       [-1.55970189, -1.59618128, -0.53250511],\n",
       "       [-1.60126634, -1.56001986, -0.53064219],\n",
       "       [-1.58786033, -1.59139247, -0.52425584],\n",
       "       [-1.53790888, -1.58222705, -0.54532522],\n",
       "       [-1.56250572, -1.57547627, -0.53874369],\n",
       "       [-1.57369975, -1.6057626 , -0.52427112],\n",
       "       [-1.57760642, -1.56469408, -0.53720501],\n",
       "       [-1.6165358 , -1.59544176, -0.51316897],\n",
       "       [-1.58011918, -1.57391131, -0.53304327],\n",
       "       [-1.57169651, -1.61932836, -0.52041174],\n",
       "       [-1.56326264, -1.57714992, -0.53787925],\n",
       "       [-1.6156187 , -1.5966684 , -0.513058  ],\n",
       "       [-1.5952138 , -1.57907708, -0.52599076],\n",
       "       [-1.57053952, -1.57454717, -0.53620203],\n",
       "       [-1.56804288, -1.57772909, -0.53596589],\n",
       "       [-1.59415129, -1.57899023, -0.52638607],\n",
       "       [-1.58957152, -1.59982603, -0.52078279],\n",
       "       [-1.57872229, -1.59773934, -0.52524897],\n",
       "       [-1.55386075, -1.55801483, -0.54815331],\n",
       "       [-1.58189038, -1.57509478, -0.53200504],\n",
       "       [-1.57779431, -1.56621338, -0.53659547],\n",
       "       [-1.56616822, -1.57586556, -0.53729341],\n",
       "       [-1.61300423, -1.61000434, -0.50945116],\n",
       "       [-1.5738352 , -1.6100768 , -0.52276508],\n",
       "       [-1.54967434, -1.60124998, -0.53436964],\n",
       "       [-1.57433883, -1.58490947, -0.53121681],\n",
       "       [-1.60819241, -1.57911635, -0.52156044],\n",
       "       [-1.55723437, -1.58827566, -0.53613589],\n",
       "       [-1.58336356, -1.59177158, -0.5256823 ],\n",
       "       [-1.59668168, -1.5888826 , -0.52209083],\n",
       "       [-1.57811218, -1.58996126, -0.52813765],\n",
       "       [-1.57702129, -1.5775633 , -0.53284488],\n",
       "       [-1.56489978, -1.57217532, -0.53905721],\n",
       "       [-1.58017962, -1.59096046, -0.5270701 ],\n",
       "       [-1.56284216, -1.58058344, -0.53681825],\n",
       "       [-1.56788376, -1.5878305 , -0.5324825 ],\n",
       "       [-1.57120412, -1.57858123, -0.53454191],\n",
       "       [-1.5315923 , -1.57157269, -0.55149034],\n",
       "       [-1.56915753, -1.58711837, -0.53227847],\n",
       "       [-1.60248195, -1.65491843, -0.49840917],\n",
       "       [-1.61036814, -1.632285  , -0.5030169 ],\n",
       "       [-1.6157353 , -1.62037779, -0.50512263],\n",
       "       [-1.6169441 , -1.6181349 , -0.50546092],\n",
       "       [-1.61366288, -1.63651742, -0.50056768],\n",
       "       [-1.63432144, -1.61537301, -0.50071227],\n",
       "       [-1.60321647, -1.63853876, -0.50337324],\n",
       "       [-1.63429763, -1.6041921 , -0.50441499],\n",
       "       [-1.6412005 , -1.61278265, -0.49935744],\n",
       "       [-1.59066057, -1.63683158, -0.50814019],\n",
       "       [-1.58084405, -1.62066194, -0.51679182],\n",
       "       [-1.61521923, -1.62066934, -0.50519709],\n",
       "       [-1.60354381, -1.62368304, -0.50808517],\n",
       "       [-1.61676049, -1.640937  , -0.49813845],\n",
       "       [-1.59828169, -1.66036835, -0.49809483],\n",
       "       [-1.58527659, -1.64028619, -0.50885365],\n",
       "       [-1.6077101 , -1.61327723, -0.51012499],\n",
       "       [-1.60212552, -1.61651792, -0.51092036],\n",
       "       [-1.65644506, -1.62511766, -0.49054047],\n",
       "       [-1.63760757, -1.60451406, -0.50324093],\n",
       "       [-1.59845981, -1.63244754, -0.50693044],\n",
       "       [-1.59627473, -1.6410839 , -0.50487659],\n",
       "       [-1.64577277, -1.60979309, -0.49888461],\n",
       "       [-1.6018356 , -1.61407389, -0.51182814],\n",
       "       [-1.59624927, -1.62554947, -0.50992388],\n",
       "       [-1.61209495, -1.60323858, -0.51201469],\n",
       "       [-1.59295063, -1.61502511, -0.51451654],\n",
       "       [-1.58647522, -1.6170363 , -0.51605889],\n",
       "       [-1.61834864, -1.63309272, -0.50013353],\n",
       "       [-1.61655694, -1.59183841, -0.5143858 ],\n",
       "       [-1.63134235, -1.60648479, -0.50460873],\n",
       "       [-1.59434854, -1.60237192, -0.51828498],\n",
       "       [-1.6171793 , -1.63847085, -0.49878934],\n",
       "       [-1.6072865 , -1.59757849, -0.51553126],\n",
       "       [-1.63728202, -1.59973115, -0.50494097],\n",
       "       [-1.61727196, -1.62037452, -0.50461811],\n",
       "       [-1.58508241, -1.64782607, -0.50649961],\n",
       "       [-1.60296588, -1.61476698, -0.51121841],\n",
       "       [-1.58263844, -1.61838617, -0.5169287 ],\n",
       "       [-1.59483632, -1.62063928, -0.51201628],\n",
       "       [-1.59920593, -1.64142403, -0.50378531],\n",
       "       [-1.58159447, -1.62894452, -0.51380242],\n",
       "       [-1.61036814, -1.632285  , -0.5030169 ],\n",
       "       [-1.60540758, -1.63541458, -0.50365019],\n",
       "       [-1.59194431, -1.64595067, -0.50477507],\n",
       "       [-1.59081642, -1.6343332 , -0.50889682],\n",
       "       [-1.61502986, -1.62136806, -0.50503058],\n",
       "       [-1.59480719, -1.62232812, -0.51146943],\n",
       "       [-1.578976  , -1.64369281, -0.5099113 ],\n",
       "       [-1.59475285, -1.62263603, -0.51138645]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对数函数的特性可以放大小概率值的惩罚效果，而压缩大概率值的贡献。例如：\n",
    "\n",
    "如果模型对正确类别的预测概率很高（如 0.9），\n",
    "−\n",
    "log\n",
    "\n",
    "(\n",
    "0.9\n",
    ")\n",
    "≈\n",
    "0.1\n",
    "−log(0.9)≈0.1，表示惩罚较小。\n",
    "如果模型对正确类别的预测概率很低（如 0.01），\n",
    "−\n",
    "log\n",
    "⁡\n",
    "(\n",
    "0.01\n",
    ")\n",
    "≈\n",
    "4.6\n",
    "−log(0.01)≈4.6，表示惩罚较大。\n",
    "通过对概率取对数，模型可以更敏感地优化低概率错误，推动其改进。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.1527015161352128)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过交叉熵损失来计算损失函数：交叉熵损失用于衡量两个概率分布之间的差异\n",
    "loss = -np.sum(y_one_hot * np.log(prediction))  # 固有公式: 交叉熵损失\n",
    "loss = (\n",
    "    loss / n_samples\n",
    ")  # 运用于梯度求解时, 除以样本数, 能够平均化损失, 确保梯度更新的稳定性和一致性。\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算梯度\n",
    "alpha = 0.1  # 学习率，用于控制每次更新的步长\n",
    "lambd = 0.01  # L1正则化参数，用于防止过拟合，\n",
    "\n",
    "\"\"\"实验证明，\n",
    "1. 去掉L1正则化后, 函数拟合的快, 在相同的迭代次数时, 准确率更高, 但随着迭代次数增加时, A值就成了Nan,发生了过拟合, A系数变极端化。\n",
    "2. 加上L1正则化后, 函数拟合的慢, 在相同的迭代次数时, 准确率较低，但随着迭代次数增加时, A值不会变成Nan值, 也就是不会发生极端变化。\n",
    "3. 正则化有效的原因, 假如A为正数, A - lambd * A, A会加快变化; 假如A为负数, A - lambd * A, A会减慢变化\n",
    "\"\"\"\n",
    "\n",
    "for i in range(10000):\n",
    "\n",
    "    # 计算预测值\n",
    "    prediction = np.exp(X_1 @ A) / np.sum(np.exp(X_1 @ A), axis=1, keepdims=True)\n",
    "\n",
    "    # 梯度整体 + lambd * A\n",
    "    grad = (X_1.T @ (prediction - y_one_hot) / n_samples + lambd * A)  # 除以样本数, 确保梯度更新的稳定性和一致性。\n",
    "\n",
    "    \"\"\"\n",
    "    在将偏置项融入特征矩阵时，通常会在特征矩阵的最后一列添加一个全为 1 的列, 注意是最后一列.\n",
    "    这样，偏置项对应的权重就成为了参数矩阵 𝐴 的第 -1 行\n",
    "    \"\"\"\n",
    "    grad[-1, :] = (grad[-1, :] - lambd * A[-1, :])  # 去掉正则化对偏置项的影响, 故偏置项 - lambd * A\n",
    "\n",
    "    # 更新权重参数\n",
    "    A = A - alpha * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = X_1 @ A  # 注意这里的参数矩阵，必须转置才能进行矩阵相乘\n",
    "\n",
    "# softmax函数将结果归一化，以此概率化\n",
    "sum_exp = np.sum(np.exp(result), axis=1, keepdims=True)\n",
    "prediction = np.exp(result) / sum_exp\n",
    "prediction = pd.DataFrame(data=prediction, columns=np.unique(y))\n",
    "prediction = prediction.idxmax(axis=1)\n",
    "\n",
    "# 计算准确率\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 与sigmoid回归只预测 **正类概率** 不同, softmax回归同时 **预测标签的每一个类的概率**\n",
    "\n",
    "2. 因此softmax的y要做onehot编码, 系数矩阵A的初始为: **A = np.random.rand(n_features, n_classes)**\n",
    "\n",
    "3. 线性回归的梯度下降公式:  **gradients = X_1.T @ (X_1 @ A - y) / n**  (最小二乘)\n",
    "\n",
    "4. 非线性回归梯度下降公式:  **gradients = X_1.T @ (X_1 @ A - y) / n**  (最小二乘)\n",
    "\n",
    "5. Lasso回归梯度下降公式:   **gradients = X_1.T @ (X_1 @ A - y) / n + lambda_val * np.sign(A)**  (最小二乘)\n",
    "\n",
    "6. Ridge回归梯度下降公式:   **gradients = X_1.T @ (X_1 @ A - y) / n + 2*lambda_val * A**  (最小二乘)\n",
    "\n",
    "7. 逻辑sigmoid回归梯度下降公式:    **gradient = np.dot(X_1.T, (y_pre - y)) / n**   (最小二乘)\n",
    "\n",
    "8. softmax回归梯度下降公式:  **grad = X_1.T@(prediction - y_one_hot ) / n_samples + lambd * A**  (交叉熵损失)\n",
    "\n",
    "9. 从上可以看出, 梯度下降公式的一般原则:**X_1.T @ (预测值 - 真实值 )**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
