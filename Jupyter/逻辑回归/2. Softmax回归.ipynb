{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 理论知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 不同于逻辑回归sigmoid函数，**y = 1/(1+e^(-(ax+b)))**，softmax函数预测多个概率，对应y属于每一类别的概率，如y=[0.7,0.2,0.1]\n",
    "2. 对应的softmax公式如下：\n",
    "3. ![Example Image](img/softmax公式1.jpg)\n",
    "4. https://blog.csdn.net/xu380393916/article/details/102496419"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1 导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>花萼长度</th>\n",
       "      <th>花萼宽度</th>\n",
       "      <th>花瓣长度</th>\n",
       "      <th>花瓣宽度</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     花萼长度  花萼宽度  花瓣长度  花瓣宽度\n",
       "0     5.1   3.5   1.4   0.2\n",
       "1     4.9   3.0   1.4   0.2\n",
       "2     4.7   3.2   1.3   0.2\n",
       "3     4.6   3.1   1.5   0.2\n",
       "4     5.0   3.6   1.4   0.2\n",
       "..    ...   ...   ...   ...\n",
       "145   6.7   3.0   5.2   2.3\n",
       "146   6.3   2.5   5.0   1.9\n",
       "147   6.5   3.0   5.2   2.0\n",
       "148   6.2   3.4   5.4   2.3\n",
       "149   5.9   3.0   5.1   1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入模块\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 处理数据集\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "y = pd.DataFrame(data=iris.target, columns=[\"label\"])\n",
    "\n",
    "# 特征重命名\n",
    "map_ = {\n",
    "    \"sepal length (cm)\": \"花萼长度\",\n",
    "    \"sepal width (cm)\": \"花萼宽度\",\n",
    "    \"petal length (cm)\": \"花瓣长度\",\n",
    "    \"petal width (cm)\": \"花瓣宽度\",\n",
    "}\n",
    "X = X.rename(columns=map_)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. softmax回归基于线性回归y=ax+b，y = e^(ax+b)/sum(e^(ax+b))，**添加xn+1列=1**，以融合b到参数A，有y = e^(X·A)/sum(e^(X·A))\n",
    "\n",
    "2. 由于softmax输出y对应每个标签的概率，如[0.7, 0.2, 0.1], 那么原始标签y也要处理成这样的形式，故此需要onehot编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意！！一定要对数据进行归一化，统一量纲，不然梯度下降大概率无法收敛！！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# 数据归一化！！！！非常重要！！！！\n",
    "X_1 = Normalizer().fit_transform(X)\n",
    "\n",
    "# 添加补充列\n",
    "X_1 = np.hstack((X_1, np.ones((len(X_1), 1))))\n",
    "\n",
    "# 将y array化，方便后续计算，矩阵乘法@符号，和A.dot(B)形式都要求为array，否则只能用np.dot(A,B)\n",
    "y = y.values\n",
    "\n",
    "# 对标签进行one-hot编码,[0. 0. 0. 1.]，即每个标签在对应标签处为1，其余为0\n",
    "n_classes = np.unique(y).shape[0]\n",
    "n_samples = X_1.shape[0]\n",
    "n_features = X_1.shape[1]\n",
    "\n",
    "y_one_hot = np.zeros((n_samples, n_classes))\n",
    "y_one_hot[np.arange(n_samples), y.T] = 1\n",
    "y_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 算法实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 梯度下降\n",
    "**非凸函数，不可正规方程求解**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先，能够理解希望函数能输出每一个类别的概率，故此softmax函数有归一化操作：**X_1@A / np.sum(X_1@A,axis=1,keepdims=True)**\n",
    "\n",
    "2. 为什么要将**X_1@A**指数化呢？**y = np.exp(X_1@A) / np.sum(np.exp(X_1@A),axis=1,keepdims=True)** \n",
    "\n",
    "3. 这么做主要有两个原因\n",
    "\n",
    "    1. 通过将输出指数化，将假设函数的输出转换为正值。这对于创建一个有效的概率分布是必需的，因为概率值不能为负。\n",
    "    \n",
    "    2. 指数函数曲线呈现递增趋势，最重要的是斜率逐渐增大，也就是说在x轴上一个很小的变化，可以导致y轴上很大的变化。这有助于在计算概率时清晰地区分那些具有更高原始逻辑值的类别。\n",
    "\n",
    "4. 交叉熵损失: ![Example Image](img/交叉熵损失1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.109164191430863)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "对理论开始验证, 看下softmax函数每步操作的结果是否具有必要性以及原理性\n",
    "\"\"\"\n",
    "\n",
    "# 1. 首先，将y分别输出三类的概率，这样的思路是没问题的, 那么，初始化系数A\n",
    "A = np.random.rand(n_features, n_classes)\n",
    "\n",
    "# 很显然X·A的操作并不能实现输出概率化(3类加起来 = 1)\n",
    "X_1 @ A\n",
    "\n",
    "# 归一化：有效果\n",
    "X_1 @ A / np.sum(X_1 @ A, axis=1, keepdims=True)\n",
    "\n",
    "# 根据实验，不做e的指数化不可行，尽管归一化能得到总和为1，但是特殊的A会导致预测值为负数(尽管3类别的预测值相加仍为1)，不能得到我们想要的概率。\n",
    "prediction = np.exp(X_1 @ A) / np.sum(np.exp(X_1 @ A), axis=1, keepdims=True)\n",
    "\n",
    "# 通过交叉熵损失来计算损失函数：交叉熵损失用于衡量两个概率分布之间的差异\n",
    "loss = -np.sum(y_one_hot * np.log(prediction))  # 固有公式: 交叉熵损失\n",
    "loss = (\n",
    "    loss / n_samples\n",
    ")  # 运用于梯度求解时, 除以样本数, 能够平均化损失, 确保梯度更新的稳定性和一致性。\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算梯度\n",
    "alpha = 0.1  # 学习率，用于控制每次更新的步长\n",
    "lambd = 0.01  # L1正则化参数，用于防止过拟合，\n",
    "\n",
    "\"\"\"实验证明，\n",
    "1. 去掉L1正则化后, 函数拟合的快, 在相同的迭代次数时, 准确率更高, 但随着迭代次数增加时, A值就成了Nan,发生了过拟合, A系数变极端化。\n",
    "2. 加上L1正则化后, 函数拟合的慢, 在相同的迭代次数时, 准确率较低，但随着迭代次数增加时, A值不会变成Nan值, 也就是不会发生极端变化。\n",
    "3. 正则化有效的原因, 假如A为正数, A - lambd * A, A会加快变化; 假如A为负数, A - lambd * A, A会减慢变化\n",
    "\"\"\"\n",
    "\n",
    "for i in range(5000):\n",
    "\n",
    "    # 计算预测值\n",
    "    prediction = np.exp(X_1 @ A) / np.sum(np.exp(X_1 @ A), axis=1, keepdims=True)\n",
    "\n",
    "    # 梯度整体 + lambd * A\n",
    "    grad = (\n",
    "        X_1.T @ (prediction - y_one_hot) / n_samples + lambd * A\n",
    "    )  # 除以样本数, 确保梯度更新的稳定性和一致性。\n",
    "\n",
    "    \"\"\"\n",
    "    在将偏置项融入特征矩阵时，通常会在特征矩阵的最后一列添加一个全为 1 的列, 注意是最后一列.\n",
    "    这样，偏置项对应的权重就成为了参数矩阵 𝐴 的第 -1 行\n",
    "    \"\"\"\n",
    "    grad[-1, :] = (\n",
    "        grad[-1, :] - lambd * A[-1, :]\n",
    "    )  # 去掉正则化对偏置项的影响, 故偏置项 - lambd * A\n",
    "\n",
    "    # 更新权重参数\n",
    "    A = A - alpha * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = X_1 @ A  # 注意这里的参数矩阵，必须转置才能进行矩阵相乘\n",
    "\n",
    "# softmax函数将结果归一化，以此概率化\n",
    "sum_exp = np.sum(np.exp(result), axis=1, keepdims=True)\n",
    "prediction = np.exp(result) / sum_exp\n",
    "prediction = pd.DataFrame(data=prediction, columns=np.unique(y))\n",
    "prediction = prediction.idxmax(axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 与sigmoid回归只预测 **正类概率** 不同, softmax回归同时 **预测标签的每一个类的概率**\n",
    "\n",
    "2. 因此softmax的y要做onehot编码, 系数矩阵A的初始为: **A = np.random.rand(n_features, n_classes)**\n",
    "\n",
    "3. 线性回归的梯度下降公式:  **gradients = X_1.T @ (X_1 @ A - y) / n**  (最小二乘)\n",
    "\n",
    "4. 非线性回归梯度下降公式:  **gradients = X_1.T @ (X_1 @ A - y) / n**  (最小二乘)\n",
    "\n",
    "5. Lasso回归梯度下降公式:   **gradients = X_1.T @ (X_1 @ A - y) / n + lambda_val * np.sign(A)**  (最小二乘)\n",
    "\n",
    "6. Ridge回归梯度下降公式:   **gradients = X_1.T @ (X_1 @ A - y) / n + 2*lambda_val * A**  (最小二乘)\n",
    "\n",
    "7. 逻辑sigmoid回归梯度下降公式:    **gradient = np.dot(X_1.T, (y_pre - y)) / n**   (最小二乘)\n",
    "\n",
    "8. softmax回归梯度下降公式:  **grad = X_1.T@(prediction - y_one_hot ) / n_samples + lambd * A**  (交叉熵损失)\n",
    "\n",
    "9. 从上可以看出, 梯度下降公式的一般原则:**X_1.T @ (预测值 - 真实值 )**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
